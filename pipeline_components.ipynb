{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Components\n",
    "\n",
    "This Jupyter notebook shows some example usages of\n",
    "NLP components in different frameworks, in particular\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- ([scikit-learn](https://scikit-learn.org/stable/))\n",
    "\n",
    "adapted from: \n",
    "- https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "- https://www.geeksforgeeks.org/python-nlp-analysis-of-restaurant-reviews/?ref=lbp\n",
    "- https://www.nltk.org/book/ch07.html\n",
    "- https://github.com/DistrictDataLabs/intro-to-nltk/blob/master/NLTK.ipynb\n",
    "- https://buildmedia.readthedocs.org/media/pdf/nltk/latest/nltk.pdf\n",
    "- https://spacy.io/usage/spacy-101 \n",
    "- https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing/\n",
    "- https://www.machinelearningplus.com/spacy-tutorial-nlp/ \n",
    "- https://realpython.com/natural-language-processing-spacy-python/ \n",
    "- https://stackabuse.com/python-for-nlp-working-with-the-gensim-library-part-1/ \n",
    "\n",
    "\n",
    "For package installation guidelines please visit the respective websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stands for the Natural Language Toolkit and is written by two eminent computational linguists, Steven Bird (Senior Research Associate of the LDC and professor at the University of Melbourne) and Ewan Klein (Professor of Linguistics at Edinburgh University). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. \n",
    "\n",
    "In this notebook, you are going to outline the following topics:\n",
    "- sentence tokenizer\n",
    "- word tokenizer \n",
    "- part-of-speech tags \n",
    "- named entity recognition \n",
    "- stopwords \n",
    "- stemming and lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenizer\n",
    "`sent_tokenize`: a Punkt sentence tokenizer (Return a sentence-tokenized copy of text)\n",
    "\n",
    "This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used. The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
    "\n",
    "However, Punkt is designed to learn parameters (a list of abbreviations, etc.) unsupervised from a corpus similar to the target domain. The pre-packaged models may therefore be unsuitable: use PunktSentenceTokenizer(text) to learn parameters from the given text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your NLTK version is 3.5.\n",
      "\n",
      "Sentence splitter:\n",
      "Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa.\n",
      "He briefly attended the University of Pretoria before moving to Canada when he was 17 to attend Queen's University.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print('Your NLTK version is {}.\\n'.format(nltk.__version__))\n",
    "\n",
    "text_0 = \"Musk was born to a Canadian mother and South African \" \\\n",
    "           \"father and raised in Pretoria, South Africa. He briefly \" \\\n",
    "           \"attended the University of Pretoria before moving to Canada \" \\\n",
    "           \"when he was 17 to attend Queen's University.\"\n",
    "\n",
    "print('Sentence splitter:')\n",
    "for sent in sent_tokenize(text_0):\n",
    "    print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizer\n",
    "\n",
    "`word_tokenize`: a Treebank tokenizer\n",
    "\n",
    "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This is the method that is invoked by word_tokenize(). It assumes that the text has already been segmented into sentences, e.g., using sent_tokenize(). In our case we just feed in the entire paragraph. \n",
    "This tokenizer performs the following steps:\n",
    "\n",
    "    - split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n",
    "    - treat most punctuation characters as separate tokens\n",
    "    - split off commas and single quotes, when followed by whitespace\n",
    "    - separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "Musk\n",
      "was\n",
      "born\n",
      "to\n",
      "a\n",
      "Canadian\n",
      "mother\n",
      "and\n",
      "South\n",
      "African\n",
      "father\n",
      "and\n",
      "raised\n",
      "in\n",
      "Pretoria\n",
      ",\n",
      "South\n",
      "Africa\n",
      ".\n",
      "He\n",
      "briefly\n",
      "attended\n",
      "the\n",
      "University\n",
      "of\n",
      "Pretoria\n",
      "before\n",
      "moving\n",
      "to\n",
      "Canada\n",
      "when\n",
      "he\n",
      "was\n",
      "17\n",
      "to\n",
      "attend\n",
      "Queen\n",
      "'s\n",
      "University\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print('\\nTokens:')\n",
    "tokens = nltk.word_tokenize(text_0)\n",
    "for t in tokens:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech tags \n",
    "`pos_tag`: a maximum entropy tagger trained on the Penn Treebank\n",
    "\n",
    "A “tag” is a case-sensitive string that specifies some property of a token, such as its part of speech. Tagged tokens are encoded as tuples (tag, token). For example, the following tagged token combines the word 'born' with a verb part of speech tag ('VBN').\n",
    "\n",
    "\n",
    "There are several other taggers including (notably) the BrillTagger as well as the BrillTrainer to train your own tagger or tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part-of-Speech (for first five tokens):\n",
      "('Musk', 'NNP')\n",
      "('was', 'VBD')\n",
      "('born', 'VBN')\n",
      "('to', 'TO')\n",
      "('a', 'DT')\n"
     ]
    }
   ],
   "source": [
    "# in case of an error, uncomment and run the following line to install the tagger\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print('\\n\\nPart-of-Speech (for first five tokens):')\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "for t in tagged[0:5]:\n",
    "    print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on.\n",
    "NLTK provides a classifier that has already been trained to recognize named entities on the Penn Treebank, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\n",
    "You can also retrain the chunker if you'd like - the code is very readable to extend it with a Gazette or otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Named Entities (based on noun phrase chunking):\n",
      "PERSON Musk\n",
      "GPE Canadian\n",
      "GPE South African\n",
      "GPE Pretoria\n",
      "GPE South Africa\n",
      "ORGANIZATION University\n",
      "GPE Pretoria\n",
      "GPE Canada\n",
      "PERSON Queen\n",
      "ORGANIZATION University\n"
     ]
    }
   ],
   "source": [
    "#incase of error, uncomment and run the following line to install the need components \n",
    "#nltk.download('words')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "\n",
    "print('\\n\\nNamed Entities (based on noun phrase chunking):')\n",
    "for sent in nltk.sent_tokenize(text_0):\n",
    "   for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "      if hasattr(chunk, 'label'):\n",
    "         print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "One of the common preprocessing method is removal of stopwords from the corpus, NLTK offers a list of stopwords in different languagaes. This list may vary across different libraries, and you might want to add or remove some of them based on your application domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopwords:\n",
      "NLTK includes 179 stopwords.\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"] ...\n"
     ]
    }
   ],
   "source": [
    "print('\\nStopwords:')\n",
    "# depending on how you installed an initialized NLTK\n",
    "# you might want to include the command\n",
    "# nltk.download('stopwords')\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print('NLTK includes {} stopwords.'.format(len(nltk_stopwords)))\n",
    "print(nltk_stopwords[0:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization \n",
    "We have an immense number of word forms, e.g., plural, verb tenses - it is helpful for many applications to normalize these word forms into some canonical word for further exploration. In English (and many other languages) - mophological context indicate gender, tense, quantity, etc. but these subtleties might not be necessary:\n",
    "\n",
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions, since these handle a wide range of irregular cases. Here we look at two stemming techniques, porter and lancaster: the major difference between the porter and lancaster stemming algorithms is that the lancaster stemmer is significantly more aggressive than the porter stemmer. \n",
    "Let's have a brief look at the stemmer.\n",
    "See, e.g., https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "Lemmatization reduces words to their base word, which is linguistically a correct lemma. It transforms a root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. A stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma.\n",
    "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   WordNet lemmatizer  \n",
      "Musk                musk                musk                Musk                \n",
      "was                 wa                  was                 wa                  \n",
      "born                born                born                born                \n",
      "to                  to                  to                  to                  \n",
      "a                   a                   a                   a                   \n",
      "Canadian            canadian            canad               Canadian            \n",
      "mother              mother              moth                mother              \n",
      "and                 and                 and                 and                 \n",
      "South               south               sou                 South               \n",
      "African             african             afr                 African             \n",
      "father              father              fath                father              \n",
      "and                 and                 and                 and                 \n",
      "raised              rais                rais                raised              \n",
      "in                  in                  in                  in                  \n",
      "Pretoria            pretoria            pretor              Pretoria            \n",
      ",                   ,                   ,                   ,                   \n",
      "South               south               sou                 South               \n",
      "Africa              africa              afric               Africa              \n",
      ".                   .                   .                   .                   \n",
      "He                  He                  he                  He                  \n",
      "briefly             briefli             brief               briefly             \n",
      "attended            attend              attend              attended            \n",
      "the                 the                 the                 the                 \n",
      "University          univers             univers             University          \n",
      "of                  of                  of                  of                  \n",
      "Pretoria            pretoria            pretor              Pretoria            \n",
      "before              befor               bef                 before              \n",
      "moving              move                mov                 moving              \n",
      "to                  to                  to                  to                  \n",
      "Canada              canada              canad               Canada              \n",
      "when                when                when                when                \n",
      "he                  he                  he                  he                  \n",
      "was                 wa                  was                 wa                  \n",
      "17                  17                  17                  17                  \n",
      "to                  to                  to                  to                  \n",
      "attend              attend              attend              attend              \n",
      "Queen               queen               queen               Queen               \n",
      "'s                  's                  's                  's                  \n",
      "University          univers             univers             University          \n",
      ".                   .                   .                   .                   \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "#incase of an error, uncomment the following line\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\", \"WordNet lemmatizer\"))\n",
    "for word in nltk.word_tokenize(text_0):\n",
    "    print(\"{0:20}{1:20}{2:20}{3:20}\".format(word,porter.stem(word),lancaster.stem(word),lemmatizer.lemmatize(word)))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "spaCy is a free, open-source library spaCy for Natural Language Processing developed by Matthew Honnibal and Ines Montani. The factors that work in the favor of spaCy are the set of features it offers, the ease of use, and the fact that the library is always kept up-to-date.\n",
    "\n",
    "The first step for a text string, when working with spaCy, is to pass it to an NLP object. This object is essentially a pipeline of several text pre-processing operations through which the input text string has to go through. When you call `nlp` on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. \n",
    "\n",
    "[pipline]: https://d33wubrfki0l68.cloudfront.net/16b2ccafeefd6d547171afa23f9ac62f159e353d/48b91/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg \"\"\n",
    "![pipline][pipline]\n",
    "\n",
    "As you can see in the figure above, the NLP pipeline has multiple components, such as tokenizer, tagger, parser, ner, etc. So, the input text string has to go through all these components before we can work on it.\n",
    "You can use the below code to figure out the active pipeline components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spaCy version is 2.3.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "print('Your spaCy version is {}.\\n'.format(spacy.__version__))\n",
    "\n",
    "# load the English model and assign it to the object nlp\n",
    "# to download the model : python -m spacy download en_core_web_sm \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not need the entire pipeline you can disable parts of it using: `nlp.disable_pipes('tagger', 'parser')`, which disables `tagger` and `parser`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenizer\n",
    "Spacy’s pretrained neural models provide sentence segmentation via syntactic dependency parsers. It also provides a rule-based Sentencizer, which will be very likely to fail for more complex sentences. The component is also available via the string name \"sentencizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The sentences are:\n",
      "The attorney general has previously been supportive of Trump's unfounded claims about voter fraud, and this latest move comes during an incredibly tense time and could inflame an already fraught transition.\n",
      "President-elect Joe Biden is beginning his transition into office while Trump and his administration refuse to recognize the former vice president's victory, making baseless claims about voter fraud and illegal votes that threaten to undermine the bedrock of American government.\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"\"\"The attorney general has previously been supportive of Trump's unfounded claims about voter fraud, and this latest move comes during an incredibly tense time and could inflame an already fraught transition. President-elect Joe Biden is beginning his transition into office while Trump and his administration refuse to recognize the former vice president's victory, making baseless claims about voter fraud and illegal votes that threaten to undermine the bedrock of American government.\"\"\"\n",
    "doc_1 = nlp(text_1)\n",
    "\n",
    "print('\\nThe sentences are:')\n",
    "for sent in doc_1.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizer\n",
    "In a first step, spaCy separates word by space and then applying some guidelines such as exception rule, prefix, suffix etc. An example is shown in the figure below:\n",
    "\n",
    "[word]: https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg \"\"\n",
    "![word][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens in the sentence:\n",
      "The\n",
      "Republican\n",
      "president\n",
      "is\n",
      "being\n",
      "challenged\n",
      "by\n",
      "Democratic\n",
      "Party\n",
      "nominee\n",
      "Joe\n",
      "Biden\n"
     ]
    }
   ],
   "source": [
    "text_2 = 'The Republican president is being challenged by Democratic Party nominee Joe Biden'\n",
    "print('\\nTokens in the sentence:')\n",
    "for token in nlp(text_2):\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech tags \n",
    "After tokenization, spaCy can parse and tag a given Doc. In spaCy, POS tags are available as an attribute on the Token object:\n",
    "\n",
    "Here, two attributes of the Token class are accessed:\n",
    "\n",
    "- `tag_` lists the fine-grained part of speech.\n",
    "- `pos_` lists the coarse-grained part of speech.\n",
    "\n",
    "`spacy.explain` gives descriptive details about a particular POS tag. spaCy provides a complete tag list along with an explanation for each tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part-of-Speech:\n",
      "The DET determiner\n",
      "Republican ADJ adjective\n",
      "president NOUN noun, singular or mass\n",
      "is AUX verb, 3rd person singular present\n",
      "being AUX verb, gerund or present participle\n",
      "challenged VERB verb, past participle\n",
      "by ADP conjunction, subordinating or preposition\n",
      "Democratic PROPN noun, proper singular\n",
      "Party PROPN noun, proper singular\n",
      "nominee NOUN noun, singular or mass\n",
      "Joe PROPN noun, proper singular\n",
      "Biden PROPN noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "print('\\nPart-of-Speech:')\n",
    "for token in nlp(text_2):\n",
    "    print(token.text,  token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "spaCy features an extremely fast statistical entity recognition system that assigns labels to contiguous spans of tokens. The default model identifies a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new examples.\n",
    "\n",
    "spaCy has the property ents on Doc objects. You can use it to extract named entities.\n",
    "In the above below, `entity` is a `Span` object with various attributes:\n",
    "- `text` gives the Unicode text representation of the entity.\n",
    "- `start_char` denotes the character offset for the start of the entity.\n",
    "- `end_char` denotes the character offset for the end of the entity.\n",
    "- `label_` gives the label of the entity.\n",
    "spacy.explain gives descriptive details about an entity label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities:\n",
      "Republican --> NORP\n",
      "Democratic Party --> ORG\n",
      "Joe Biden --> PERSON\n",
      "Barack --> GPE\n",
      "US --> GPE\n",
      "the 1970s --> DATE\n"
     ]
    }
   ],
   "source": [
    "print('\\nNamed Entities:')\n",
    "\n",
    "text_3 = \"The Republican president is being challenged by Democratic Party nominee Joe Biden, who is best known as Barack Obama’s vice-president but has been in US politics since the 1970s\"\n",
    "doc_3 = nlp(text_3)\n",
    "\n",
    "for entity in doc_3.ents:\n",
    "    print(entity.text, '-->', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords \n",
    "SpaCy has a list of its own stopwords, different from NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy contains 326 stopwords\n"
     ]
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "print('SpaCy contains {} stopwords'.format(len(stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization \n",
    "It might be surprising to you but spaCy doesn't contain any function for stemming as it relies on lemmatization only. For stemming it is better to use NLTK instead. \n",
    "spaCy has the attribute lemma_ on the Token class. This attribute has the lemmatized form of a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> the\n",
      "attorney --> attorney\n",
      "general --> general\n",
      "has --> have\n",
      "previously --> previously\n",
      "been --> be\n",
      "supportive --> supportive\n",
      "of --> of\n",
      "Trump --> Trump\n",
      "'s --> 's\n",
      "unfounded --> unfounded\n",
      "claims --> claim\n",
      "about --> about\n",
      "voter --> voter\n",
      "fraud --> fraud\n",
      "and --> and\n",
      "this --> this\n",
      "latest --> late\n",
      "move --> move\n",
      "comes --> come\n",
      "during --> during\n",
      "an --> an\n",
      "incredibly --> incredibly\n",
      "tense --> tense\n",
      "time --> time\n",
      "and --> and\n",
      "could --> could\n",
      "inflame --> inflame\n",
      "an --> an\n",
      "already --> already\n",
      "fraught --> fraught\n",
      "transition --> transition\n",
      "President --> President\n",
      "elect --> elect\n",
      "Joe --> Joe\n",
      "Biden --> Biden\n",
      "is --> be\n",
      "beginning --> begin\n",
      "his --> -PRON-\n",
      "transition --> transition\n",
      "into --> into\n",
      "office --> office\n",
      "while --> while\n",
      "Trump --> Trump\n",
      "and --> and\n",
      "his --> -PRON-\n",
      "administration --> administration\n",
      "refuse --> refuse\n",
      "to --> to\n",
      "recognize --> recognize\n",
      "the --> the\n",
      "former --> former\n",
      "vice --> vice\n",
      "president --> president\n",
      "'s --> 's\n",
      "victory --> victory\n",
      "making --> make\n",
      "baseless --> baseless\n",
      "claims --> claim\n",
      "about --> about\n",
      "voter --> voter\n",
      "fraud --> fraud\n",
      "and --> and\n",
      "illegal --> illegal\n",
      "votes --> vote\n",
      "that --> that\n",
      "threaten --> threaten\n",
      "to --> to\n",
      "undermine --> undermine\n",
      "the --> the\n",
      "bedrock --> bedrock\n",
      "of --> of\n",
      "American --> american\n",
      "government --> government\n"
     ]
    }
   ],
   "source": [
    "for token in doc_1:\n",
    "\n",
    "    if not token.is_punct:\n",
    "        print(token, '-->', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. It is a great package for processing texts, working with word vector models (such as Word2Vec, FastText etc) and for building topic models.\n",
    "Also, another significant advantage with gensim is: it lets you handle large text files without having to load the entire file in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Gensim version is 3.8.3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "print('Your Gensim version is {}.\\n'.format(gensim.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenizer\n",
    "`split_sentences` is part of the summarization code and splits text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences:\n",
      "Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa.\n",
      "He briefly attended the University of Pretoria before moving to Canada when he was 17 to attend Queen's University.\n"
     ]
    }
   ],
   "source": [
    "print('\\nSentences:')\n",
    "for sent in split_sentences(text_0):\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizer\n",
    "`tokenize` outputs tokens as unicode strings, removing accent marks and optionally lowercasing the unidoc string by assigning True to one of the parameters, lowercase, to_lower, or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "Musk\n",
      "was\n",
      "born\n",
      "to\n",
      "a\n",
      "Canadian\n",
      "mother\n",
      "and\n",
      "South\n",
      "African\n",
      "father\n",
      "and\n",
      "raised\n",
      "in\n",
      "Pretoria\n",
      "South\n",
      "Africa\n",
      "He\n",
      "briefly\n",
      "attended\n",
      "the\n",
      "University\n",
      "of\n",
      "Pretoria\n",
      "before\n",
      "moving\n",
      "to\n",
      "Canada\n",
      "when\n",
      "he\n",
      "was\n",
      "to\n",
      "attend\n",
      "Queen\n",
      "s\n",
      "University\n"
     ]
    }
   ],
   "source": [
    "print('\\nTokens:')\n",
    "for token in tokenize(text_0):\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
